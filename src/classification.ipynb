{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functions import predict, pipe, license_plate,check,thresh\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My name is Laval Jacquin, I'm 38 years old, my...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nour Kired and Damien Sonneville are good alte...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Arnaud Mimart is the head of the chocoPolice</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yeah that's what i i just did today i got uh D...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Founded in 1861, the Massachusetts Institute o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>In 1972, Phillip Morris, Inc.'s Miller Brewing...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>Here you will find plenty of copies of all loc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>Although the members of the CVR committee cons...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>412</td>\n",
       "      <td>The company's ISD works with ISD officials or ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>413</td>\n",
       "      <td>You can buy several of these to make your own ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           sentence  label\n",
       "0             0  My name is Laval Jacquin, I'm 38 years old, my...    1.0\n",
       "1             1  Nour Kired and Damien Sonneville are good alte...    1.0\n",
       "2             2       Arnaud Mimart is the head of the chocoPolice    1.0\n",
       "3             3  yeah that's what i i just did today i got uh D...    0.0\n",
       "4             4  Founded in 1861, the Massachusetts Institute o...    0.0\n",
       "..          ...                                                ...    ...\n",
       "409         409  In 1972, Phillip Morris, Inc.'s Miller Brewing...    0.0\n",
       "410         410  Here you will find plenty of copies of all loc...    0.0\n",
       "411         411  Although the members of the CVR committee cons...    0.0\n",
       "412         412  The company's ISD works with ISD officials or ...    0.0\n",
       "413         413  You can buy several of these to make your own ...    0.0\n",
       "\n",
       "[414 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv('/Users/nour/Documents/OKP4/detection-of-personal-data/data_test/personal_data_db_translated.csv')\n",
    "df=df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/414 [14:24<6:51:26, 61.72s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000002?line=0'>1</a>\u001b[0m labels\u001b[39m=\u001b[39mpii_detect(df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000002?line=1'>2</a>\u001b[0m labels\n",
      "File \u001b[0;32m~/Documents/OKP4/detection-of-personal-data/src/functions.py:40\u001b[0m, in \u001b[0;36mpii_detect\u001b[0;34m(df, thresh)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=37'>38</a>\u001b[0m res\u001b[39m=\u001b[39m[]\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m tqdm(df[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m],total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df)):\n\u001b[0;32m---> <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=39'>40</a>\u001b[0m     l\u001b[39m=\u001b[39mpredict(sent,thresh)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=40'>41</a>\u001b[0m     res\u001b[39m.\u001b[39mappend(l)\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=41'>42</a>\u001b[0m \u001b[39mreturn\u001b[39;00m(res)\n",
      "File \u001b[0;32m~/Documents/OKP4/detection-of-personal-data/src/functions.py:46\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(sentence, threshold)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(sentence : \u001b[39mstr\u001b[39m, threshold : \u001b[39mfloat\u001b[39m):\n\u001b[0;32m---> <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=45'>46</a>\u001b[0m     result_details \u001b[39m=\u001b[39m pipe(sentence, candidate_labels, multi_label\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=46'>47</a>\u001b[0m     result_details \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(result_details[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m], result_details[\u001b[39m'\u001b[39m\u001b[39mscores\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     <a href='file:///Users/nour/Documents/OKP4/detection-of-personal-data/src/functions.py?line=47'>48</a>\u001b[0m     result \u001b[39m=\u001b[39m {name : np\u001b[39m.\u001b[39mmean(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(result_details\u001b[39m.\u001b[39mget, lst))) \u001b[39mfor\u001b[39;00m lst, name \u001b[39min\u001b[39;00m pii_}\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py:181\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=177'>178</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=178'>179</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to understand extra arguments \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=180'>181</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(sequences, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1023'>1024</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1024'>1025</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1025'>1026</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py:1048\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1045'>1046</a>\u001b[0m all_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1046'>1047</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1047'>1048</a>\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1048'>1049</a>\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(model_outputs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1049'>1050</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(all_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py:943\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=940'>941</a>\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=941'>942</a>\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=942'>943</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=943'>944</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=944'>945</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py:200\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=197'>198</a>\u001b[0m sequence \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=198'>199</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_input_names}\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=199'>200</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=201'>202</a>\u001b[0m model_outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=202'>203</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcandidate_label\u001b[39m\u001b[39m\"\u001b[39m: candidate_label,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=203'>204</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m: sequence,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=204'>205</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=205'>206</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moutputs,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=206'>207</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=207'>208</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:1491\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1485'>1486</a>\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1486'>1487</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1487'>1488</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1488'>1489</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1490'>1491</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1491'>1492</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1492'>1493</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1493'>1494</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1494'>1495</a>\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1495'>1496</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1496'>1497</a>\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1497'>1498</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1498'>1499</a>\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1499'>1500</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1500'>1501</a>\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1501'>1502</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1502'>1503</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1503'>1504</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1504'>1505</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1505'>1506</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1506'>1507</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1508'>1509</a>\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:1217\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1213'>1214</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1215'>1216</a>\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1216'>1217</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1217'>1218</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1218'>1219</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1219'>1220</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1220'>1221</a>\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1221'>1222</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1222'>1223</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1223'>1224</a>\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1224'>1225</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1225'>1226</a>\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1226'>1227</a>\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:842\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=834'>835</a>\u001b[0m         layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=835'>836</a>\u001b[0m             create_custom_forward(encoder_layer),\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=836'>837</a>\u001b[0m             hidden_states,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=837'>838</a>\u001b[0m             attention_mask,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=838'>839</a>\u001b[0m             (head_mask[idx] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=839'>840</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=840'>841</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=841'>842</a>\u001b[0m         layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=842'>843</a>\u001b[0m             hidden_states,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=843'>844</a>\u001b[0m             attention_mask,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=844'>845</a>\u001b[0m             layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=845'>846</a>\u001b[0m             output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=846'>847</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=848'>849</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=850'>851</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:331\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=327'>328</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=329'>330</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=330'>331</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(hidden_states))\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=331'>332</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=332'>333</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/activations.py?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/activations.py?line=55'>56</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels=pii_detect(df)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My name is Laval Jacquin, I'm 38 years old, my...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nour Kired and Damien Sonneville are good alte...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Arnaud Mimart is the head of the chocoPolice</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence  label\n",
       "0           0  My name is Laval Jacquin, I'm 38 years old, my...    1.0\n",
       "1           1  Nour Kired and Damien Sonneville are good alte...    1.0\n",
       "2           2       Arnaud Mimart is the head of the chocoPolice    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_labels = ['First and last name', 'Mail','Internet protocol (IP) addresses', 'location', 'Date of birth', 'Date of death', 'Place of birth', 'Place of death', 'IP', 'a cookie', 'phone', 'health data','private data','personal data','not personal data','not private data']\n",
    "name=[\"person's name\",\"person\"]\n",
    "birth=[\"birthday\",\"private\"]\n",
    "# ip=[\"Internet protocol (IP) addresses\",\"IP adress\"]\n",
    "cookie=[\"cookie identifiers\",\"a cookie\"]\n",
    "phone=[\"phone numbers\",\"phone number\",\"telphone number\"]\n",
    "mail=[\"mail addresses\",\"mail address\",\"mail\"]\n",
    "location=[\"location adress\",\"private postal address\"]\n",
    "health=[\"health data\",\"medical records\",\"health information\",\"medical information\",\"Medical Procedure names\",\"Medical Procedure identification codes\"]\n",
    "personal=[\"personal data\",\"personal\",\"personal information\",\"private data\",\"private information\"]\n",
    "not_personal=[\"not personal data\",\"not personal information\",\"not personal\",\"not private data\",\"not private\",\"not private information\"]\n",
    "passport=['passport','code']\n",
    "Driving_license=['driving license','code']\n",
    "social_security_number=['social security number']\n",
    "Tax_file_number=['tax file number']\n",
    "credit_card=['credit card number']\n",
    "candidate_labels=name+cookie+phone+mail+location+health+personal+not_personal+birth+passport+Driving_license+social_security_number+Tax_file_number+credit_card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_=[(name,'person'),(birth,'birth'),(cookie,'cookie'),(phone,'phone'),(mail,'mail'),(location,'location'),(health,'health'),(personal,'personal'),\n",
    "        (not_personal,'not_personal'),(passport,\"passport\"),(Driving_license,\"Driving_license\"),(social_security_number,\"social_security_number\"),(Tax_file_number,\"Tax_file_number\"),(credit_card,\"credit_card\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(sentence : str,treshold : float):\n",
    "    result_details=pipe(sentence, candidate_labels, multi_label=True)\n",
    "    result_details=dict(zip(result_details['labels'],result_details['scores']))\n",
    "    result = {name : np.mean(list(map(result_details.get, lst))) for lst,name in pii_}\n",
    "    pii_detected={name : result[name] for name in thresh(result,treshold)}\n",
    "    pii_detected=check(pii_detected,sentence)\n",
    "    if license_plate(sentence):\n",
    "            pii_detected[\"license_plate\"]=0.9\n",
    "    if pii_detected=={}:\n",
    "            mean=0\n",
    "    else:\n",
    "            mean=np.mean(list(pii_detected.values()))\n",
    "    del result_details,result\n",
    "    return int(pii_detected!={}),mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My name is Laval Jacquin, I'm 38 years old, my...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nour Kired and Damien Sonneville are good alte...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Arnaud Mimart is the head of the chocoPolice</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yeah that's what i i just did today i got uh D...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Founded in 1861, the Massachusetts Institute o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>In 1972, Phillip Morris, Inc.'s Miller Brewing...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>Here you will find plenty of copies of all loc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>Although the members of the CVR committee cons...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>412</td>\n",
       "      <td>The company's ISD works with ISD officials or ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>413</td>\n",
       "      <td>You can buy several of these to make your own ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           sentence  label\n",
       "0             0  My name is Laval Jacquin, I'm 38 years old, my...    1.0\n",
       "1             1  Nour Kired and Damien Sonneville are good alte...    1.0\n",
       "2             2       Arnaud Mimart is the head of the chocoPolice    1.0\n",
       "3             3  yeah that's what i i just did today i got uh D...    0.0\n",
       "4             4  Founded in 1861, the Massachusetts Institute o...    0.0\n",
       "..          ...                                                ...    ...\n",
       "409         409  In 1972, Phillip Morris, Inc.'s Miller Brewing...    0.0\n",
       "410         410  Here you will find plenty of copies of all loc...    0.0\n",
       "411         411  Although the members of the CVR committee cons...    0.0\n",
       "412         412  The company's ISD works with ISD officials or ...    0.0\n",
       "413         413  You can buy several of these to make your own ...    0.0\n",
       "\n",
       "[414 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6/414 [03:20<3:47:13, 33.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000010?line=1'>2</a>\u001b[0m predicted_proba\u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m tqdm(df[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000010?line=3'>4</a>\u001b[0m     result\u001b[39m=\u001b[39mpredict(sent,\u001b[39m0.9\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000010?line=4'>5</a>\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(result[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000010?line=5'>6</a>\u001b[0m     predicted_proba\u001b[39m.\u001b[39mappend(result[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32m/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb Cell 7'\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence, treshold)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(sentence : \u001b[39mstr\u001b[39m,treshold : \u001b[39mfloat\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000009?line=1'>2</a>\u001b[0m     result_details\u001b[39m=\u001b[39mpipe(sentence, candidate_labels, multi_label\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000009?line=2'>3</a>\u001b[0m     result_details\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(result_details[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m],result_details[\u001b[39m'\u001b[39m\u001b[39mscores\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nour/Documents/OKP4/detection-of-personal-data/src/classification.ipynb#ch0000009?line=3'>4</a>\u001b[0m     result \u001b[39m=\u001b[39m {name : np\u001b[39m.\u001b[39mmean(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(result_details\u001b[39m.\u001b[39mget, lst))) \u001b[39mfor\u001b[39;00m lst,name \u001b[39min\u001b[39;00m pii_}\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py:181\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=177'>178</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=178'>179</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to understand extra arguments \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=180'>181</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(sequences, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1023'>1024</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1024'>1025</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1025'>1026</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py:1048\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1045'>1046</a>\u001b[0m all_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1046'>1047</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1047'>1048</a>\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1048'>1049</a>\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(model_outputs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=1049'>1050</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(all_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py:943\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=940'>941</a>\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=941'>942</a>\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=942'>943</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=943'>944</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py?line=944'>945</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py:200\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=197'>198</a>\u001b[0m sequence \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=198'>199</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_input_names}\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=199'>200</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=201'>202</a>\u001b[0m model_outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=202'>203</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcandidate_label\u001b[39m\u001b[39m\"\u001b[39m: candidate_label,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=203'>204</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m: sequence,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=204'>205</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=205'>206</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moutputs,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=206'>207</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/zero_shot_classification.py?line=207'>208</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:1491\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1485'>1486</a>\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1486'>1487</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1487'>1488</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1488'>1489</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1490'>1491</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1491'>1492</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1492'>1493</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1493'>1494</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1494'>1495</a>\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1495'>1496</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1496'>1497</a>\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1497'>1498</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1498'>1499</a>\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1499'>1500</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1500'>1501</a>\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1501'>1502</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1502'>1503</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1503'>1504</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1504'>1505</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1505'>1506</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1506'>1507</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1508'>1509</a>\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:1217\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1213'>1214</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1215'>1216</a>\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1216'>1217</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1217'>1218</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1218'>1219</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1219'>1220</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1220'>1221</a>\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1221'>1222</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1222'>1223</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1223'>1224</a>\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1224'>1225</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1225'>1226</a>\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=1226'>1227</a>\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:842\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=834'>835</a>\u001b[0m         layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=835'>836</a>\u001b[0m             create_custom_forward(encoder_layer),\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=836'>837</a>\u001b[0m             hidden_states,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=837'>838</a>\u001b[0m             attention_mask,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=838'>839</a>\u001b[0m             (head_mask[idx] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=839'>840</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=840'>841</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=841'>842</a>\u001b[0m         layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=842'>843</a>\u001b[0m             hidden_states,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=843'>844</a>\u001b[0m             attention_mask,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=844'>845</a>\u001b[0m             layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=845'>846</a>\u001b[0m             output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=846'>847</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=848'>849</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=850'>851</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py:333\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=330'>331</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden_states))\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=331'>332</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=332'>333</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(hidden_states)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=333'>334</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/transformers/models/bart/modeling_bart.py?line=334'>335</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/nour/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "predicted_proba=[]\n",
    "for sent in tqdm(df['sentence']):\n",
    "    result=predict(sent,0.9)\n",
    "    y_pred.append(result[0])\n",
    "    predicted_proba.append(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      My name is Laval Jacquin, I'm 38 years old, my...\n",
       "1      Nour Kired and Damien Sonneville are good alte...\n",
       "2           Arnaud Mimart is the head of the chocoPolice\n",
       "3      yeah that's what i i just did today i got uh D...\n",
       "4      Founded in 1861, the Massachusetts Institute o...\n",
       "                             ...                        \n",
       "409    In 1972, Phillip Morris, Inc.'s Miller Brewing...\n",
       "410    Here you will find plenty of copies of all loc...\n",
       "411    Although the members of the CVR committee cons...\n",
       "412    The company's ISD works with ISD officials or ...\n",
       "413    You can buy several of these to make your own ...\n",
       "Name: sentence, Length: 414, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prédit 0</th>\n",
       "      <th>prédit 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vrai 0</th>\n",
       "      <td>289</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vrai 1</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prédit 0  prédit 1\n",
       "vrai 0       289       103\n",
       "vrai 1         8        14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(df['label'], y_pred)\n",
    "cf = pd.DataFrame(conf, columns=['prédit ' + _ for _ in ['0','1']])\n",
    "cf.index = ['vrai ' + _ for _ in ['0','1']]\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7318840579710145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([df.label==y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Je suis euh, grand sergent chef, à la retraite...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The FBI legal attach�'s office in Paris first ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In Prinsengracht, Otto Frank and his family hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>He joined the rest of his team at their hotel.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Je mange réellement vite, aussi vite que je pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Davidson should not adopt the pronunciation of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Et je pensais que c'était un privilège, et ça ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>You may have heard of me.   Captain Calverley ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Pour plus de détails, voir //www.healtheffects...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Je te chercherai le 11 décembre !</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label  predicted\n",
       "8    Je suis euh, grand sergent chef, à la retraite...      0          1\n",
       "9    The FBI legal attach�'s office in Paris first ...      0          1\n",
       "15   In Prinsengracht, Otto Frank and his family hi...      0          1\n",
       "22      He joined the rest of his team at their hotel.      0          1\n",
       "24   Je mange réellement vite, aussi vite que je pe...      0          1\n",
       "..                                                 ...    ...        ...\n",
       "392  Davidson should not adopt the pronunciation of...      0          1\n",
       "394  Et je pensais que c'était un privilège, et ça ...      0          1\n",
       "403  You may have heard of me.   Captain Calverley ...      0          1\n",
       "404  Pour plus de détails, voir //www.healtheffects...      0          1\n",
       "406                  Je te chercherai le 11 décembre !      0          1\n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=df.copy()\n",
    "result['predicted']=y_pred\n",
    "result_no_match=result[result.label!=y_pred]\n",
    "result_no_match.to_csv('./data_test/result_no_match.csv')\n",
    "result_no_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [16:14<00:00,  8.78s/it]\n"
     ]
    }
   ],
   "source": [
    "result_no_match=pd.read_csv('./data_test/result_no_match.csv')\n",
    "y_pred_06=[]\n",
    "predicted_proba=[]\n",
    "for sent in tqdm(result_no_match['sentence']):\n",
    "    result=predict(sent,treshold=0.6)\n",
    "    y_pred_06.append(result[0])\n",
    "    predicted_proba.append(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44144144144144143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prédit 0</th>\n",
       "      <th>prédit 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vrai 0</th>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vrai 1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prédit 0  prédit 1\n",
       "vrai 0        49        54\n",
       "vrai 1         8         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(result_no_match['label'], y_pred_06)\n",
    "cf = pd.DataFrame(conf, columns=['prédit ' + _ for _ in ['0','1']])\n",
    "cf.index = ['vrai ' + _ for _ in ['0','1']]\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [15:27<00:00,  8.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6216216216216216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_no_match=pd.read_csv('./data_test/result_no_match.csv')\n",
    "y_pred_06=[]\n",
    "predicted_proba=[]\n",
    "for sent in tqdm(result_no_match['sentence']):\n",
    "    result=predict(sent,treshold=0.7)\n",
    "    y_pred_06.append(result[0])\n",
    "    predicted_proba.append(result[1])\n",
    "\n",
    "np.mean([result_no_match.label==y_pred_06])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prédit 0</th>\n",
       "      <th>prédit 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vrai 0</th>\n",
       "      <td>69</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vrai 1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prédit 0  prédit 1\n",
       "vrai 0        69        34\n",
       "vrai 1         8         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(result_no_match['label'], y_pred_06)\n",
    "cf = pd.DataFrame(conf, columns=['prédit ' + _ for _ in ['0','1']])\n",
    "cf.index = ['vrai ' + _ for _ in ['0','1']]\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [17:10<00:00,  9.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_no_match=pd.read_csv('./data_test/result_no_match.csv')\n",
    "y_pred_08=[]\n",
    "predicted_proba=[]\n",
    "for sent in tqdm(result_no_match['sentence']):\n",
    "    result=predict(sent,treshold=0.8)\n",
    "    y_pred_08.append(result[0])\n",
    "    predicted_proba.append(result[1])\n",
    "\n",
    "np.mean([result_no_match.label==y_pred_08])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prédit 0</th>\n",
       "      <th>prédit 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vrai 0</th>\n",
       "      <td>74</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vrai 1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prédit 0  prédit 1\n",
       "vrai 0        74        29\n",
       "vrai 1         8         0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(result_no_match['label'], y_pred_08)\n",
    "cf = pd.DataFrame(conf, columns=['prédit ' + _ for _ in ['0','1']])\n",
    "cf.index = ['vrai ' + _ for _ in ['0','1']]\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [11:43<00:00,  6.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7927927927927928"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_no_match=pd.read_csv('./data_test/result_no_match.csv')\n",
    "y_pred_75=[]\n",
    "predicted_proba=[]\n",
    "for sent in tqdm(result_no_match['sentence']):\n",
    "    result=predict(sent,treshold=0.9)\n",
    "    y_pred_75.append(result[0])\n",
    "    predicted_proba.append(result[1])\n",
    "\n",
    "np.mean([result_no_match.label==y_pred_75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prédit 0</th>\n",
       "      <th>prédit 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vrai 0</th>\n",
       "      <td>88</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vrai 1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prédit 0  prédit 1\n",
       "vrai 0        88        15\n",
       "vrai 1         8         0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(result_no_match['label'], y_pred_75)\n",
    "cf = pd.DataFrame(conf, columns=['prédit ' + _ for _ in ['0','1']])\n",
    "cf.index = ['vrai ' + _ for _ in ['0','1']]\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
