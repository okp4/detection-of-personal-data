{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des bibliothèques utiles\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from MyFunctions import processSentences, meanVector, cosineDistance\n",
    "import statistics\n",
    "# NLP Packages\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/nour/.local/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nour/.local/lib/python3.8/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/nour/.local/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /home/nour/.local/lib/python3.8/site-packages (1.27.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitz in /home/nour/.local/lib/python3.8/site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: nibabel in /home/nour/.local/lib/python3.8/site-packages (from fitz) (3.2.2)\n",
      "Requirement already satisfied: scipy in /home/nour/.local/lib/python3.8/site-packages (from fitz) (1.4.1)\n",
      "Requirement already satisfied: pandas in /home/nour/.local/lib/python3.8/site-packages (from fitz) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/nour/.local/lib/python3.8/site-packages (from fitz) (1.22.2)\n",
      "Requirement already satisfied: pyxnat in /home/nour/.local/lib/python3.8/site-packages (from fitz) (1.4)\n",
      "Requirement already satisfied: nipype in /home/nour/.local/lib/python3.8/site-packages (from fitz) (1.7.1)\n",
      "Requirement already satisfied: httplib2 in /usr/lib/python3/dist-packages (from fitz) (0.14.0)\n",
      "Requirement already satisfied: configobj in /home/nour/.local/lib/python3.8/site-packages (from fitz) (5.0.6)\n",
      "Requirement already satisfied: configparser in /home/nour/.local/lib/python3.8/site-packages (from fitz) (5.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nibabel->fitz) (45.2.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from nibabel->fitz) (21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nour/.local/lib/python3.8/site-packages (from pandas->fitz) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nour/.local/lib/python3.8/site-packages (from pandas->fitz) (2.8.2)\n",
      "Requirement already satisfied: pathlib>=1.0 in /home/nour/.local/lib/python3.8/site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: requests>=2.20 in /home/nour/.local/lib/python3.8/site-packages (from pyxnat->fitz) (2.25.0)\n",
      "Requirement already satisfied: lxml>=4.3 in /home/nour/.local/lib/python3.8/site-packages (from pyxnat->fitz) (4.8.0)\n",
      "Requirement already satisfied: six>=1.15 in /home/nour/.local/lib/python3.8/site-packages (from pyxnat->fitz) (1.16.0)\n",
      "Requirement already satisfied: future>=0.16 in /usr/lib/python3/dist-packages (from pyxnat->fitz) (0.18.2)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (6.1.1)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (3.5.1)\n",
      "Requirement already satisfied: etelemetry>=0.2.0 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: traits!=5.0,>=4.6 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (2.8)\n",
      "Requirement already satisfied: prov>=1.5.2 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (2.0.0)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /usr/lib/python3/dist-packages (from nipype->fitz) (3.16.0)\n",
      "Requirement already satisfied: click>=6.6.0 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (7.1.2)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /home/nour/.local/lib/python3.8/site-packages (from nipype->fitz) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/nour/.local/lib/python3.8/site-packages (from packaging>=14.3->nibabel->fitz) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (3.0.4)\n",
      "Requirement already satisfied: isodate in /home/nour/.local/lib/python3.8/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Requirement already satisfied: ci-info>=0.2 in /home/nour/.local/lib/python3.8/site-packages (from etelemetry>=0.2.0->nipype->fitz) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /home/nour/.local/lib/python3.8/site-packages (20220319)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/lib/python3/dist-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from pdfminer.six) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #Rational expressions\n",
    "import PyPDF2\n",
    "import io\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content : str=''\n",
    "with open(\"TousVosLivres - Bio de Madonna.pdf\", \"rb\") as pdf_file:\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    page = read_pdf.pages[0]\n",
    "    page_content += page.extractText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    '''Convert pdf content from a file path to text\n",
    "\n",
    "    :path the file path\n",
    "    '''\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "\n",
    "    with io.StringIO() as retstr:\n",
    "        with TextConverter(rsrcmgr, retstr, codec=codec,\n",
    "                           laparams=laparams) as device:\n",
    "            with open(path, 'rb') as fp:\n",
    "                interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "                password = \"\"\n",
    "                maxpages = 0\n",
    "                caching = True\n",
    "                pagenos = set()\n",
    "\n",
    "                for page in PDFPage.get_pages(fp,\n",
    "                                              pagenos,\n",
    "                                              maxpages=maxpages,\n",
    "                                              password=password,\n",
    "                                              caching=caching,\n",
    "                                              check_extractable=True):\n",
    "                    interpreter.process_page(page)\n",
    "\n",
    "                return retstr.getvalue()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text=convert_pdf_to_txt(\"TousVosLivres - Bio de Madonna.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177029cb75c241c3a82cacf335b96fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='bookTitle', options=('TousVosLivres - Vie et almanach de B-Frankli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#List of books along with their identifier on Project Gutenberg\n",
    "# Feel free to expand the list and pick your favorite\n",
    "books = { \"TousVosLivres - Vie et almanach de B-Franklin\": \"TousVosLivres - Vie et almanach de B-Franklin.pdf\",\n",
    "          \"TousVosLivres - Bio de Madonna\": \"TousVosLivres - Bio de Madonna.pdf\",\n",
    "          \"TousVosLivres - bio de Steve Jobs\": \"TousVosLivres - bio de Steve Jobs.pdf\",\n",
    "        }\n",
    "\n",
    "def bookSelection(bookTitle):\n",
    "    bookId = books[bookTitle]\n",
    "    print( \"Loading data\" )\n",
    "    book = bookTitle+'.pdf'\n",
    "    words = re.split('[^A-Za-z]+', convert_pdf_to_txt(book))\n",
    "    words = [w for w in words if w!='']\n",
    "    text_madonna=' '.join(words)\n",
    "    return text_madonna\n",
    "\n",
    "interact(bookSelection, bookTitle=books.keys() );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "treatement for book: TousVosLivres - bio de Steve Jobs\n",
      "Loading data\n",
      "pii_res\n",
      "{'Person': ['mme Albert Einstein', 'mme Albert Einstein', 'mme Mais Apple', 'mme Mark Zuckerberg Voil', 'mme Product Red', 'mme Tim Cook', 'mme Laurene Ses', ' Palo Alto est', ' En Steve Jobs a', ' Joanne Carole Schieble a', ' Lorsque Steve a', ' Patty Trois a', ' Paul Jobs Palo Alto Paul est', ' Bill Gates a', ' Ronald Wayne a', ' Larry Brilliant a', ' Larry Brilliant a', ' Larry Brilliant a', ' Combi Wozniak a', ' HP Combi Wozniak a', ' Byte Shop Il est', ' Jobs Apple Computer Il est', ' National Semiconductor a', ' National Semiconductor a', ' National Semiconductor a', ' Apple II est', ' Xerox PARC Pour a', ' John Sculley a', ' NeXT Une NeXTstation a', ' NeXT Computer est', ' NeXT Cube est', ' World Wide Web a', ' George Lucas San Rafael a', ' Steve Jobs est', ' Pixar Disney a', ' Gil Amelio est', ' Gil Amelio est', ' Mac OS a', ' Mac OS a', ' Steve Jobs a', ' Steve Jobs est', ' Steve Jobs a', ' En Forbes est', ' Steve Jobs est', ' App Store a', ' Steve Jobs est', ' Intel Il a', ' Steve Jobs est', ' Mais Apple est', ' Dell Les a', ' IBM Il est', ' Google Eric Schmidt a', ' Steve Jobs a', ' Steve Jobs a', ' Steve Jobs est', ' Joan Baez a', ' Bob Dylan Steve a', ' Steve Jobs est', ' Steve Jobs a', ' Steve Jobs a', ' Macworld Conference a', ' Steve Jobs a', ' Steve Jobs a', ' Mark Zuckerberg a', ' Steve Jobs a', ' Steve Jobs est', ' Steve Lohr Creating Jobs a', ' Smithsonian Oral a', ' Video Histories Steve Jobs a', ' My Life a', ' Apple CEO Steve Jobs a', ' The Huffington Post a', ' The New Strait Times a', ' Leslie Iwerks To Infinity a', ' Pixar History a', ' The Academy Awards Database a', ' Vanity Fair a', ' Pixar Emeryville a', ' The Los Angeles Times a', ' Steve Jobs a', ' All Things Digital a', ' The Wall Street Journal a', ' The New Yorker a', ' Mark Duell a', ' The Boston Globe a', ' The New York Times a', ' The Washington Post a', ' The Guardian a', ' CNN Money Fortune a', ' CNN Money Fortune a', ' CNN Money Fortune a', ' The Little Kingdom a', ' Steve Jobs a', ' Bill Gates Historic Interview a', ' Steve Jobs a', ' Stanford University a', ' Steve Jobs Commencement a', ' Ars Technica a', ' Mac History a', ' Bill Gates a', ' CNN Money Fortune a', ' Bill Gates a', ' The New York Times a', ' The Hindu a', ' Steve Jobs a', ' US Government Patent Database a', ' Steve Jobs a', ' Bono Praises Steve Jobs a', ' Reunion Never Came a', ' Steve Jobs a', ' The Sun a', ' Steve Jobs a', ' The New York Times a', ' The Daily Mail a', ' Ars Technica a', ' Information Week a', ' Apple Insider a', ' The New York Times a', ' Steve Jobs Orbituary a', ' Cupertino City Concil Apple Press Info a', ' Remembering Steve a', ' The Los Angeles Times a', ' Steve Jobs a', ' The Daily Mail a', ' Steve Jobs a', ' Innovation Recipients a', ' The Newyorker a', ' The New York Times a', ' The Economist a', ' The Guardian a', ' Dan Gillmor Steve Jobs a', ' Les Echos a'], 'Adresse': [('One', 'Farm', 'o', 'le', 'jeune', 'Steve', 'se')]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import os\n",
    "from src.pii import labs_df\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "# print('')\n",
    "# print('test_df = pd.read_csv(\"test_file_small_sample.csv\")')\n",
    "# test_df = pd.read_csv(\"test_file_small_sample.csv\")\n",
    "# print('pii_res = count_labels(test_df)')\n",
    "# pii_res = count_labels(test_df)\n",
    "# print('pii_res')\n",
    "# print(pii_res)\n",
    "# print('')\n",
    "book='TousVosLivres - bio de Steve Jobs'\n",
    "print('')\n",
    "print(f'treatement for book: {book}')\n",
    "test_df = pd.DataFrame({bookSelection(book)})\n",
    "pii_res = pd.DataFrame(labs_df(test_df)).iloc[0,0]\n",
    "print('pii_res')\n",
    "print(pii_res)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_madonna=pd.DataFrame(pii_res['Person'])\n",
    "df_madonna['label']='Person'\n",
    "df_madonna.to_csv('SteveJobs.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nlp_pipeline(text):\n",
    "    text = text.lower() # mettre les mots en minuscule\n",
    "# Retirons les caractères spéciaux :\n",
    "    text = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", text)\n",
    "    text = re.sub(r\"\\&\\S*\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Boolean Series key will be reindexed to match DataFrame index. [1013155979.py:9]\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "reddit = pd.read_csv(\"Franklin_afterProcess.csv\")\n",
    "reddit=reddit[reddit['Truth_Value']==1]\n",
    "\n",
    "reddit2 = pd.read_csv(\"madonna_afterProcess.csv\")\n",
    "reddit2=reddit2[reddit2['Truth_Value']==1]\n",
    "\n",
    "reddit3 = pd.read_csv(\"SteveJobs_afterProcess.csv\")\n",
    "reddit3=reddit[reddit3['Truth_Value']==1]\n",
    "sentences_raw = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Benjamin Franklin est'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reddit['0'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reddit)):\n",
    "    body = list(reddit['0'])[0]\n",
    "    if isinstance(body, str):\n",
    "        sentences_raw.extend(body.split('.'))\n",
    "for i in range(len(reddit2)):\n",
    "  title = list(reddit2['0'])[0]\n",
    "  if isinstance(title, str):\n",
    "      sentences_raw.append(title)\n",
    "\n",
    "for i in range(len(reddit3)):\n",
    "    title = list(reddit3['0'])[0]\n",
    "    if isinstance(title, str):\n",
    "      sentences_raw.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "'''\n",
    "Clean String\n",
    "'''\n",
    "def cleanString(s : str):\n",
    "\n",
    "    s = s.lower()\n",
    "\n",
    "    # remove emojis\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    s = regrex_pattern.sub(r'',s)\n",
    "\n",
    "    # remove non-alphabet characters\n",
    "    myPunc = '!\"#%&$()*+-./:;,<=>?@[\\\\]^_`{|}~'\n",
    "    s = s.translate(s.maketrans(myPunc, ' '*len(myPunc)))\n",
    "    s = s.translate(s.maketrans(string.digits, ' '*len(string.digits)))\n",
    "\n",
    "    # clean up extra white-space\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    return s.strip()\n",
    "\n",
    "'''\n",
    "Find Bigrams\n",
    "'''\n",
    "def findBigrams(corpus : list, min_count : int):\n",
    "\n",
    "    corpusPhrased = []\n",
    "    # this actually finds the bigrams\n",
    "    phrases = Phrases(corpus, min_count = min_count, delimiter='_')\n",
    "\n",
    "    for sent in corpus:\n",
    "        phrased = phrases[sent]\n",
    "        corpusPhrased.append(phrased)\n",
    "\n",
    "    return corpusPhrased\n",
    "\n",
    "'''\n",
    "Process Sentences\n",
    "'''\n",
    "def processSentences(sentences : list, minStringSize = 5, minTokenCount = 3, splitonPeriod = False, phraseMinCount = 25, bigrams=True, trigrams=True):\n",
    "\n",
    "    # apply string cleaning\n",
    "    sentences_clean = sentences\n",
    "\n",
    "    # lets filter out any short strings, as they likely wont hold enough information.\n",
    "    sentences_clean = list(filter(lambda x: len(x) > minStringSize, sentences_clean))\n",
    "\n",
    "    # okay so now lets go ahead and tokenize our sentences into words.\n",
    "    # were going to ignore any sentences with fewer than 3 words.\n",
    "\n",
    "    sentences_tokenized = []\n",
    "\n",
    "    for sentence in sentences_clean:\n",
    "\n",
    "        # for some of the larger bodies of text we are first going to split on periods to get 'real' sentences.\n",
    "        if splitonPeriod:\n",
    "            sents_split = sentence.split('.')\n",
    "\n",
    "            for sent in sents_split:\n",
    "                tokens = sent.split(' ')\n",
    "                # were only going to care about 3 or greater.\n",
    "                if len(tokens) >= minTokenCount:\n",
    "\n",
    "                    sentences_tokenized.append(tokens)\n",
    "\n",
    "        # dont split on period.\n",
    "        else:\n",
    "            tokens = sentence.split(' ')\n",
    "            if len(tokens) >= minTokenCount:\n",
    "                sentences_tokenized.append(tokens)\n",
    "\n",
    "\n",
    "    # Time to find Bigrams and Trigrams\n",
    "    if bigrams:\n",
    "        sentences_tokenized = findBigrams(sentences_tokenized, phraseMinCount)\n",
    "    if trigrams:\n",
    "        sentences_tokenized = findBigrams(sentences_tokenized, phraseMinCount)\n",
    "\n",
    "    return sentences_tokenized\n",
    "\n",
    "# Now lets go ahead and apply our string processing to our raw sentences.\n",
    "sentences_clean = processSentences(sentences_raw, minStringSize = 5, minTokenCount = 3, splitonPeriod = True, phraseMinCount = 25, bigrams=True, trigrams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 810)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import text8 corpus\n",
    "text8 = api.load('text8')\n",
    "\n",
    "# start with text8 as a base.\n",
    "model = Word2Vec(text8, min_count=2, window =3, sg = 1, vector_size=100,workers=5)\n",
    "\n",
    "# we can then continue to train on our model on the sentences we have created.\n",
    "model.build_vocab(sentences_clean, update=True)\n",
    "model.train(sentences_clean,total_examples=len(sentences_clean), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701, 10000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray, dot, array, float32 as REAL\n",
    "\n",
    "\n",
    "'''\n",
    "FUNCTION : meanVector(...)\n",
    "INPUT :\n",
    "        keyedVectors : word vectors or keyed vectors from gensim model, (model.wv)\n",
    "        positive : list of words or vectors to be applied positively [default = list()]\n",
    "        negative : list of words or vectors to be applied negatively [default = list()]\n",
    "OUTPUT :\n",
    "        averaged word vector, [type = numpy.ndarray]\n",
    "DESCRIPTION :\n",
    "        allows for simple averaging of positive and negative words and vectors given a gensim model's word vector library.\n",
    "        NOTE: this code is pulled from gensim's word2vec repo, I just edited it to return the averaged vector.\n",
    "'''\n",
    "\n",
    "KEY_TYPES = (str, int, np.integer)\n",
    "\n",
    "def meanVector(keyedVectors, positive=list(), negative=list()):\n",
    "\n",
    "    # remove any words that arent in the vocabulary\n",
    "    positive = list(filter(lambda x: (x in keyedVectors), positive))\n",
    "    negative = list(filter(lambda x: (x in keyedVectors), negative))\n",
    "\n",
    "    positive = [\n",
    "            (item, 1.0) if isinstance(item, KEY_TYPES + (ndarray,))\n",
    "            else item for item in positive\n",
    "            ]\n",
    "    negative = [\n",
    "            (item, -1.0) if isinstance(item, KEY_TYPES + (ndarray,))\n",
    "            else item for item in negative\n",
    "            ]\n",
    "\n",
    "    # compute the weighted average of all keys\n",
    "    all_keys, mean = set(), []\n",
    "    for key, weight in positive + negative:\n",
    "        if isinstance(key, ndarray):\n",
    "            mean.append(weight * key)\n",
    "        else:\n",
    "            mean.append(weight * keyedVectors.get_vector(key, norm=True))\n",
    "            if keyedVectors.has_index_for(key):\n",
    "                all_keys.add(keyedVectors.get_index(key))\n",
    "        if not mean:\n",
    "            raise ValueError(\"cannot compute similarity with no input\")\n",
    "\n",
    "    mean = matutils.unitvec(array(mean).mean(axis=0)).astype(REAL)\n",
    "\n",
    "    return mean\n",
    "\n",
    "'''\n",
    "FUNCTION : cosineDistance(...)\n",
    "'''\n",
    "def cosineDistance(v1, v2):\n",
    "    # matutils.unitvec scales the vectors\n",
    "    return dot(matutils.unitvec(v1), matutils.unitvec(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2616544"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1='madonna',w2='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myVec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58539/1947685395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosineDistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_vline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_dash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myVec' is not defined"
     ]
    }
   ],
   "source": [
    "sims = []\n",
    "\n",
    "for v in model.wv.vectors:\n",
    "    sims.append(cosineDistance(v, myVec))\n",
    "fig = px.histogram(pd.DataFrame({'value':sims}), x=\"value\")\n",
    "fig.add_vline(x=0.55, line_width=3, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.update_xaxes(title_text=\"Similarity\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kclusterer = KMeansClusterer(2, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "# clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "# print (clusters)\n",
    "\n",
    "# for index, phrase in enumerate(phrases):\n",
    "#     print (str(clusters[index]) + \":\" + str(phrase))\n",
    "\n",
    "# kmeans = cluster.KMeans(n_clusters=2)\n",
    "# kmeans.fit(X)\n",
    "# labels = kmeans.labels_\n",
    "# centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On commence par utiliser notre pipeline définie plus haut\n",
    "\n",
    "# phrases_propres = []\n",
    "\n",
    "# for phrase in phrases:\n",
    "#     phrases_propres.append(nlp_pipeline(phrase))\n",
    "\n",
    "# # Nous devons séparer les phrases en liste de mots\n",
    "\n",
    "# phrases_split = []\n",
    "\n",
    "# for phrase in phrases_propres:\n",
    "#     phrases_split.append(phrase.split(\" \"))\n",
    "\n",
    "# # C'est là que Word2vec intervient\n",
    "# X = []\n",
    "\n",
    "# for phrase in phrases_split:\n",
    "#     vec_phrase = []\n",
    "#     for mot in phrase:\n",
    "#         vec_phrase.append(w2v.wv[mot])\n",
    "#     X.append(np.mean(vec_phrase,0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
